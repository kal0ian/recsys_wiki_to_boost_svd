{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "\n",
    "from surprise import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "USERS_COUNT = 943\n",
    "ITEMS_COUNT = 1682\n",
    "THRESHOLD = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    data = pd.DataFrame(Dataset.load_builtin(\"ml-100k\").raw_ratings)\n",
    "    data[0] = pd.to_numeric(data[0]) - 1\n",
    "    data[1] = pd.to_numeric(data[1]) - 1\n",
    "    del data[3]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_similarities():\n",
    "    similarities = pd.read_csv(\"artificial_ratings.csv\")\n",
    "    similarities['0'] = similarities['0'] - 1\n",
    "    similarities['1'] = similarities['1'] - 1\n",
    "\n",
    "    similarities_arr = np.zeros((ITEMS_COUNT, ITEMS_COUNT))\n",
    "    for _, row in similarities.iterrows():\n",
    "        if int(row['0']) != int(row['1']):\n",
    "            similarities_arr[int(row['0']), int(row['1'])] = row['2']\n",
    "        \n",
    "    return similarities_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = load_similarities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "artificial = np.zeros((USERS_COUNT, ITEMS_COUNT))\n",
    "for item in range(ITEMS_COUNT):\n",
    "    for user in range(USERS_COUNT):\n",
    "        rating = 0\n",
    "        all_user_ratings = train_set[train_set[:, 0] == user]\n",
    "        sum_sim = 0\n",
    "        for u, i, r in all_user_ratings:\n",
    "            sim1 = similarities[item, int(i)]\n",
    "            if (sim1 < 0.05):\n",
    "                continue\n",
    "            rating = rating + r * sim1\n",
    "            sum_sim = sum_sim + sim1\n",
    "        if sum_sim == 0:\n",
    "            artificial[user, item] = -1\n",
    "        else:\n",
    "            artificial[user, item] = rating / sum_sim\n",
    "\n",
    "dataframe = pd.DataFrame(columns=[0, 1, 2])\n",
    "artificial_data = []\n",
    "for item in range(ITEMS_COUNT):\n",
    "    for user in range(USERS_COUNT):\n",
    "        if (artificial[user, item] != -1):\n",
    "            artificial_data.append([user, item, artificial[user, item]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_data = data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparcitiy: 0.9968\n"
     ]
    }
   ],
   "source": [
    "train_set, _ = train_test_split(actual_data, test_size=0.95)\n",
    "sparcity = 1 - (len(train_set) / (USERS_COUNT * ITEMS_COUNT))\n",
    "print(\"Sparcitiy: %.4f\" % sparcity)\n",
    "train_set, test_set = train_test_split(train_set, test_size=.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "artificial_data = np.array(artificial_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "artificial_data_l = np.column_stack((artificial_data, np.zeros(artificial_data.shape[0])))\n",
    "train_set_l = np.column_stack((artificial_data, np.ones(artificial_data.shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "labled_train_set = np.vstack((artificial_data_l, train_set_l))\n",
    "np.random.shuffle(labled_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing epoch 0\n",
      "Processing epoch 1\n",
      "Processing epoch 2\n",
      "Processing epoch 3\n",
      "Processing epoch 4\n",
      "Processing epoch 5\n",
      "Processing epoch 6\n",
      "Processing epoch 7\n",
      "Processing epoch 8\n",
      "Processing epoch 9\n",
      "Processing epoch 10\n",
      "Processing epoch 11\n",
      "Processing epoch 12\n",
      "Processing epoch 13\n",
      "Processing epoch 14\n",
      "Processing epoch 15\n",
      "Processing epoch 16\n",
      "Processing epoch 17\n",
      "Processing epoch 18\n",
      "Processing epoch 19\n"
     ]
    }
   ],
   "source": [
    "algo = SVD(verbose=True)\n",
    "algo.fit(labled_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEst\n",
      "Test\n",
      "TEST\n",
      "Processing epoch 0\n",
      "Processing epoch 1\n",
      "Processing epoch 2\n",
      "Processing epoch 3\n",
      "Processing epoch 4\n",
      "Processing epoch 5\n",
      "Processing epoch 6\n",
      "Processing epoch 7\n",
      "Processing epoch 8\n",
      "Processing epoch 9\n",
      "Processing epoch 10\n",
      "Processing epoch 11\n",
      "Processing epoch 12\n",
      "Processing epoch 13\n",
      "Processing epoch 14\n",
      "Processing epoch 15\n",
      "Processing epoch 16\n",
      "Processing epoch 17\n",
      "Processing epoch 18\n",
      "Processing epoch 19\n"
     ]
    }
   ],
   "source": [
    "algo = SVD(verbose=True)\n",
    "algo.fit(np.array(artificial_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST\n",
      "Processing epoch 0\n",
      "Processing epoch 1\n",
      "Processing epoch 2\n",
      "Processing epoch 3\n",
      "Processing epoch 4\n",
      "Processing epoch 5\n",
      "Processing epoch 6\n",
      "Processing epoch 7\n",
      "Processing epoch 8\n",
      "Processing epoch 9\n",
      "Processing epoch 10\n",
      "Processing epoch 11\n",
      "Processing epoch 12\n",
      "Processing epoch 13\n",
      "Processing epoch 14\n",
      "Processing epoch 15\n",
      "Processing epoch 16\n",
      "Processing epoch 17\n",
      "Processing epoch 18\n",
      "Processing epoch 19\n"
     ]
    }
   ],
   "source": [
    "algo = SVD(verbose=True)\n",
    "algo.fit(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [(r_ui_trans, algo.estimate(uid, iid)) for (uid, iid, r_ui_trans) in test_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1563097910479472"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.sqrt(mean_squared_error(predictions[:, 0], predictions[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [(r_ui_trans, algo.estimate(uid, iid)) for (uid, iid, r_ui_trans) in actual_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "from __future__ import (absolute_import, division, print_function,\n",
    "                        unicode_literals)\n",
    "USERS_COUNT = 943\n",
    "ITEMS_COUNT = 1682\n",
    "cimport numpy as np  # noqa\n",
    "import numpy as np\n",
    "from six.moves import range\n",
    "\n",
    "from surprise.prediction_algorithms.predictions import PredictionImpossible\n",
    "from surprise.utils import get_rng\n",
    "\n",
    "\n",
    "class SVD():\n",
    "\n",
    "    def __init__(self, n_factors=100, n_epochs=20, biased=True, init_mean=0,\n",
    "                 init_std_dev=.1, lr_all=.005,\n",
    "                 reg_all=.02, lr_bu=None, lr_bi=None, lr_pu=None, lr_qi=None,\n",
    "                 reg_bu=None, reg_bi=None, reg_pu=None, reg_qi=None,\n",
    "                 random_state=None, verbose=False):\n",
    "\n",
    "        self.n_factors = n_factors\n",
    "        self.n_epochs = n_epochs\n",
    "        self.biased = biased\n",
    "        self.init_mean = init_mean\n",
    "        self.init_std_dev = init_std_dev\n",
    "        self.lr_bu = lr_bu if lr_bu is not None else lr_all\n",
    "        self.lr_bi = lr_bi if lr_bi is not None else lr_all\n",
    "        self.lr_pu = lr_pu if lr_pu is not None else lr_all\n",
    "        self.lr_qi = lr_qi if lr_qi is not None else lr_all\n",
    "        self.reg_bu = reg_bu if reg_bu is not None else reg_all\n",
    "        self.reg_bi = reg_bi if reg_bi is not None else reg_all\n",
    "        self.reg_pu = reg_pu if reg_pu is not None else reg_all\n",
    "        self.reg_qi = reg_qi if reg_qi is not None else reg_all\n",
    "        self.random_state = random_state\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def fit(self, trainset):\n",
    "        self.trainset = trainset\n",
    "        \n",
    "        self.sgd(trainset)\n",
    "\n",
    "    def sgd(self, trainset):\n",
    "        artificial_lr = (sum(trainset[:,3]==0)/sum(trainset[:,3]==1))* self.lr_bu\n",
    "        # user biases\n",
    "        cdef np.ndarray[np.double_t] bu\n",
    "        # item biases\n",
    "        cdef np.ndarray[np.double_t] bi\n",
    "        # user factors\n",
    "        cdef np.ndarray[np.double_t, ndim=2] pu\n",
    "        # item factors\n",
    "        cdef np.ndarray[np.double_t, ndim=2] qi\n",
    "\n",
    "        cdef int u, i, f\n",
    "        cdef double r, err, dot, puf, qif\n",
    "        cdef global_mean = trainset[:, 2].mean()\n",
    "        self.global_mean = global_mean\n",
    "\n",
    "        cdef double lr_bu = self.lr_bu\n",
    "        cdef double lr_bi = self.lr_bi\n",
    "        cdef double lr_pu = self.lr_pu\n",
    "        cdef double lr_qi = self.lr_qi\n",
    "\n",
    "        cdef double reg_bu = self.reg_bu\n",
    "        cdef double reg_bi = self.reg_bi\n",
    "        cdef double reg_pu = self.reg_pu\n",
    "        cdef double reg_qi = self.reg_qi\n",
    "\n",
    "        rng = get_rng(self.random_state)\n",
    "\n",
    "        bu = np.zeros(USERS_COUNT, np.double)\n",
    "        bi = np.zeros(ITEMS_COUNT, np.double)\n",
    "        pu = rng.normal(self.init_mean, self.init_std_dev,\n",
    "                        (USERS_COUNT, self.n_factors))\n",
    "        qi = rng.normal(self.init_mean, self.init_std_dev,\n",
    "                        (ITEMS_COUNT, self.n_factors))\n",
    "\n",
    "        if not self.biased:\n",
    "            global_mean = 0\n",
    "\n",
    "        for current_epoch in range(self.n_epochs):\n",
    "            if self.verbose:\n",
    "                print(\"Processing epoch {}\".format(current_epoch))\n",
    "            for u, i, r, label in trainset:\n",
    "                if not label:\n",
    "                    # compute current error\n",
    "                    dot = 0  # <q_i, p_u>\n",
    "                    for f in range(self.n_factors):\n",
    "                        dot += qi[i, f] * pu[u, f]\n",
    "                    err = r - (global_mean + bu[u] + bi[i] + dot)\n",
    "\n",
    "                    # update biases\n",
    "                    if self.biased:\n",
    "                        bu[u] += lr_bu * (err - reg_bu * bu[u])\n",
    "                        bi[i] += lr_bi * (err - reg_bi * bi[i])\n",
    "\n",
    "                    # update factors\n",
    "                    for f in range(self.n_factors):\n",
    "                        puf = pu[u, f]\n",
    "                        qif = qi[i, f]\n",
    "                        pu[u, f] += lr_pu * (err * qif - reg_pu * puf)\n",
    "                        qi[i, f] += lr_qi * (err * puf - reg_qi * qif)\n",
    "                else:\n",
    "                    \n",
    "                    # compute current error\n",
    "                    dot = 0  # <q_i, p_u>\n",
    "                    for f in range(self.n_factors):\n",
    "                        dot += qi[i, f] * pu[u, f]\n",
    "                    err = r - (global_mean + bu[u] + bi[i] + dot)\n",
    "\n",
    "                    # update biases\n",
    "                    if self.biased:\n",
    "                        bu[u] += artificial_lr * (err - reg_bu * bu[u])\n",
    "                        bi[i] += artificial_lr * (err - reg_bi * bi[i])\n",
    "\n",
    "                    # update factors\n",
    "                    for f in range(self.n_factors):\n",
    "                        puf = pu[u, f]\n",
    "                        qif = qi[i, f]\n",
    "                        pu[u, f] += artificial_lr * (err * qif - reg_pu * puf)\n",
    "                        qi[i, f] += artificial_lr * (err * puf - reg_qi * qif)\n",
    "\n",
    "        self.bu = bu\n",
    "        self.bi = bi\n",
    "        self.pu = pu\n",
    "        self.qi = qi\n",
    "\n",
    "    def estimate(self, u, i):\n",
    "        # Should we cythonize this as well?\n",
    "\n",
    "        known_user = u in self.trainset[:, 0]\n",
    "        known_item = i in self.trainset[:, 1]\n",
    "        \n",
    "        u = int(u)\n",
    "        i = int(i)\n",
    "\n",
    "        if self.biased:\n",
    "            est = self.global_mean\n",
    "\n",
    "            if known_user:\n",
    "                est += self.bu[u]\n",
    "\n",
    "            if known_item:\n",
    "                est += self.bi[i]\n",
    "\n",
    "            if known_user and known_item:\n",
    "                est += np.dot(self.qi[i], self.pu[u])\n",
    "\n",
    "        else:\n",
    "            if known_user and known_item:\n",
    "                est = np.dot(self.qi[i], self.pu[u])\n",
    "            else:\n",
    "                raise PredictionImpossible('User and item are unkown.')\n",
    "\n",
    "        est = min(5.0, est)\n",
    "        est = max(1.0, est)\n",
    "        return est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
